---
title: "Linear Models Week 2 Quiz"
author: "Nate Olson"
date: "2/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. 
Consider regression through the origin of two vectors: (outcome) $\tilde{Y}=(I−1_nJ_{n,n})Y$ and (predictor) $\tilde{X}=(I−1_nJ_{n,n})X$ for $n×1$ vectors $X$ and $Y$ and $J_{n,n}$ an $n×n$ matrix of ones. What is the regression through the origin slope of $\tilde{X}$ (predictor) on $\tilde{Y}$ (outcome)? Check all that apply.

- Correct Answer: The correlation between X and Y times the ratio of the standard deviations (sd(Y) / sd(X)).

If you mean center your variables, then the regression through the origin simplifies to this.  

- $<X,Y>/<X,X>$
Slope where X as predictor and Y and outcome


- Correct Answer: $<\tilde{X},\tilde{Y}>/<\tilde{X},\tilde{X}>$
Slope of the regression of X and Y is the same as the regression through the origin for $\tilde{X}$ (predictor) on $\tilde{Y}$ (outcome).


## 2. 
Imagine that I mean center and divide equal length vectors $X$ and $Y$ by their standard deviations. Let $\tilde{X}$ and $\tilde{Y}$ be the centered and scaled versions of $X$ and $Y$. The regression through the origin slope is the same regardless of whether $\tilde{X}$ or $\tilde{Y}$ is the outcome or predictor?

Why does this example not workout?
```{r}
X <- sample(1:20, 5)
Y <- sample(1:20, 5)
tildeX <- (X - mean(X))/sd(X)
tildeY <- (Y - mean(Y))/sd(Y)
```

$\tilde{X}$ (outcome) $\tilde{Y}$ (predictor) 
```{r}
(tildeX %*% tildeY)/(tildeX %*% tildeX)
```

$\tilde{Y}$ (outcome) $\tilde{X}$ (predictor) 
```{r}
(tildeY %*% tildeX)/(tildeY %*% tildeY)
```


- Correct Answer: True  
Recall that for centered variables, the regression through the origin slope is the correlation times the ratio of standard deviations. However, the standard deviations of the scaled variables is one. 

Note $||\tilde{X}|| = ||\tilde{Y}||$ 

- False

## 3. 
Consider regression through the origin of a response Y on a predictor X that is not constant. The residuals $(e=Y−X\frac{<Y,X>}{<X,X>})$ must sum to zero?

- True  
Definition of residuals, but only when the intercept is included in the model. 

- Correct Answer: False

The residuals satisfy $<e,X>=0$. If $X$ isn't constant, then they don't have to sum to zero. 

## 4. 
Consider regression through the origin of a response Y on a predictor X. $<e,X> =0$ where $e$ is the residual $(e=Y−X\frac{<Y,X>}{<X,X>})$?

```{r}
fit <- lm(Y~X)
resid(fit) %*% X
```


- Correct Answer: True

Recall the projection argument and this is clear

- False

## 5. 
Let
```{r}
X = c(-1.45, 2.28, -1.41, 1.05, -0.61)
Y = c(-1.52, 3.83, -1.90, 1.79, -0.42)
```

what is the regression through the origin slope estimate with Y as the outcome and X as the predictor?
Mean center X and Y
```{r}
tildeX <- X - mean(X)
tildeY <- Y - mean(Y)
```


Solution
```{r}
sum(X * Y) / sum(X*X)
```

```{r}
lm(Y ~ X + 0)
```


- 0.3971

- Correct Answer: 1.463

- Correct Answer: 1.469