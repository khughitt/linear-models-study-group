---
output:
  html_document:
    toc: true
  md_document:
    variant: markdown_github
    md_extensions: +tex_math_dollars
    pandoc_args: ['--webtex']
    toc: true
    toc_depth: 2
---

```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.path='img/',
               fig.width=1080/120,
               fig.height=1080/120,
               dpi=120, fig.retina=2)
```

<h1>Linear Algebra Review XI</h1>

- Keith Hughitt
- Sept 02, 2016

Determinants
============

Some geometric motivation
-------------------------

### 2x2 matrices

A $2 \times 2$ matrix, $A$, can be viewed as a **linear map** which transforms
the **standard basis vectors** (unit vectors pointing in the direction of the
coordinate exes, in this case $e_x = (1,0)$ and $e_y = (0, 1)$) to the rows (or
columns) of $A$.

The image of the basis vectors form a parallelogram representing the
transformation applied to the unit square:

![](img/800px-Area_parallellogram_as_determinant.svg.png)

(source: [Wikipedia](https://en.wikipedia.org/wiki/Determinant#/media/File:Area_parallellogram_as_determinant.svg))

> The parallelogram defined by the rows of the above matrix is the one with
> vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the
> accompanying diagram. -Wikipedia

And:

**The absolute value of ad âˆ’ bc is the area of the parallelogram, and thus
represents the scale factor by which areas are transformed by A.** -Wikipedia

....This is the definition of the determinant ($\Delta$) of a $2 \times 2$ matrix:

![](img/determinant.png)

### 3x3 matrices

The same idea can be extended into higher dimensions, for example, the absolute
value of the determinant of a $3 \times 3$ matrix is the volume of the
**parallelepiped** bound by coordinates related to values from the original matrix.

![](img/950px-Determinant_parallelepiped.svg.png)

(source: [Wikipedia](https://en.wikipedia.org/wiki/Determinant#/media/File:Determinant_parallelepiped.svg))

Here, the parallelepiped represents the transformed **unit cube**.

### Related geometric interpretation

Another useful way of thinking of the determinant geometrically:

> You can derive the algebraic properties from this geometrical interpretation.
> For example, if two of the columns are linearly dependent, your box is
> missing a dimension and so it's been flattened to have zero volume. [2]

### Determinants of correlation matrices

For example, suppose we are looking at _correlation matrices_ for two
variables.

In the first scenario, suppose the variables are highly correlated. In this
case, the parallelogram corresponding to the matrix would be thin, and
therefor, have a small area:

```{r cor_matrix_determinant_a}
set.seed(1)

x <- 1:5
y <- x + rnorm(5)
mat <- cbind(x, y)

cor_mat1 <- cor(mat)
cor_mat1

# components of mat
a1 <- cor_mat1[1,1]
b1 <- cor_mat1[2,1]
c1 <- cor_mat1[1,2]
d1 <- cor_mat1[2,2]

plot(c(0, 2), c(0, 2), type = "n")
polygon(c(0, a1, a1 + c1, c1), c(0, b1, b1 + d1, d1), col='#369bed')

det(cor_mat1)
eigen(cor_mat1)
```

Next, suppose we have to completely uncorrelated variables; here, the
parallelogram would be much closer to the unit square and its area
would close to 1:

```{r cor_matrix_determinant_b}
x <- rnorm(100)
y <- rnorm(100)
mat <- cbind(x, y)

cor_mat2 <- cor(mat)
cor_mat2

# components of mat
a2 <- cor_mat2[1,1]
b2 <- cor_mat2[2,1]
c2 <- cor_mat2[1,2]
d2 <- cor_mat2[2,2]

plot(c(0, 2), c(0, 2), type = "n")
polygon(c(0, a2, a2 + c2, c2), c(0, b2, b2 + d2, d2), col='#369bed')

det(cor_mat2)
eigen(cor_mat2)
```

<span style='color:red;'>**QUESTION**:</span> _Why are the values
of the eigenvectors so similar for the two examples?.._

The same basic trends hold true when applied to covariance matrices as well
(correlated variables lead to closer values in the covariance matrix), however
the exact values of the coordinates and the determinant change.

Other useful theorems we missed in chapter 3...
-----------------------------------------------

- **Theorem 2** If $A$ is a triangular matrix, then det $A$ is the product of
  the entries on the main diagonal of $A$.
    - This is easy to see for $2 \times 2$ example: second product includes
      something below the diagonal so it goes to zero, leaving only the product
      coming from the diagonal.
- **Theorem 3**: Determinant for one square matrix can be determined by
  following simple rules relating to row operations used to get from that
  matrix to a new one (see p192 for details).
- Relation to invertibility of a matrix:
    1. When $A$ is invertible: det $A = (-1)^r \times (\text{product of pivots in } U)$
    2. When $A$ is _not_ invertible, det $A = 0$
- **Theorem 4** A square matrix $A$ is invertible if and only if det $A \neq 0$
    - The columns of $A$ are linearly dependent $\Rightarrow det A = 0$
        - ...remember the geometric interpretation of this idea above.
    - The rows of $A$ are linearly dependent $\implies  det A = 0$
        - Rows of $A$ = columns of $A^T$.
        - When $A^T$ is singular, so is $A$.
- **Theorem 5** If $A$ is an $n \times n$ matrix, then $\text{det} A^T = \text{det} A$.
- **Theorem 6** If $A$ and $B$ are $n \times n$ matrices, then det $A B$ =
  (det $A$)(det $B$).
  - Note: this is _not_ true for sums of determinants!

Skipping 3.3 on Cramer's Rule, etc. (too much typing and not really needed
below..)

The Characteristic Equation
===========================

Overview
--------

**Main idea:**

<div style='background-color: #ebffd4; padding: 10px; border: 1px solid #ccc;'>
Information about the <i>eigenvalues</i> of a square matrix are encoded in a
special equation called the <b>characteristic equation</b> of the matrix.
</div>

For a given square matrix, $A$, recall that eigenvalues for the matrix are
defined as:

$$
Ax = \lambda x
$$

We can then rearrange this to:

$$
(A - \lambda I)x = 0
$$

to find the eigenvalues associated with a given matrix. All values for
$\lambda$ that lead to a nontrival solution for the above equation are
the eigenvalues for the matrix.

Next, recall that for a matrix to be invertible, equation $Ax = 0$ must have
_only_ the trivial solution (IVT).

Therefore, another way to think about the above problem is to look for all
$\lambda$ such that $A - \lambda I$ is _not_ invertible.

From Lay 3.2, theorem 4 states:

> A square matrix A is invertible if and only if det A != 0

Which leads us to the following way of determining the eigenvalues for $A$:

$$
det(A - \lambda I) = 0
$$

This is called the **The Characteristic Equation** of $A$. Any scalar $\lambda$
is an eigenvalue of $A$ if and only if it satisfies the above equation.

When we have a triangular matrix, we can just take the product each of the
diagonal values - $\lambda$, and set it equal to zero.

After expansion, this leaves us with a simple polynomial equation in a single
variable!

Characteristic polynomials
--------------------------

<div style='background-color: #ebffd4; padding: 10px; border: 1px solid #ccc;'>
- If $A$ is an $n \times n$ matrix, then det ( $A - \lambda I$ )is a polynomial
of degree $n$ called the <b>characteristic polynomial</b> of $A$.
- To find the eigenvalues for $A$, we simply find the roots of its
  characteristic polynomial equation!
</div>

Similarity
----------

Another key idea from 5.2 which leads us into the chapter on matrix
diagonalization is _similarity_.

- If $A$ and $B$ are $n \times n$ matrices, then $A$ _is similar to_ $B$ if
  there is an invertible matrix $P$ such that $A = PBP^-1$.
- $A$ _is similar to_ $B$ implies $B$ _is similar to_ $A$
- Changing $A$ into $PBP^-1$ is called a **similarity transformation**

<div style='background-color: #ebffd4; padding: 10px; border: 1px solid #ccc;'>
If $n \times n$ matrices $A$ and $B$ are similar, then they have the same
characteristic polynomial and hence the same eigenvalues (with the same
multiplicities).
</div>

-note: _similarity_ $\neq$ _row equivalence_.

Diagonalization
===============

Overview
--------

**Main idea:**

<div style='background-color: #ebffd4; padding: 10px; border: 1px solid #ccc;'>
The eigenvalue-eigenvector information contained within a matrix $A$ can be
displayed  in a usefu factorization of the form $A = PDP^-1$, where:

- P -> eigenvectors
- D -> eigenvalues
</div>

- $D$ stands for _diagonal_
- Useful for computing $A^k$ for large values of $k$
- Aka "Eigendecomposition" or "spectral decomposition"
- [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) extends
  this idea to rectangular matrices.

The Diagonalization Theorem
---------------------------

- A square matrix $A$ is said to be **diagonalizable** if $A$ is similar to a
  diagonal matrix, i.e. if $A = PDP^-1$ for some invertible matrix $P$ and some
  diagonal matrix $D$.

**Theorem 5**

> An $n \times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$
> linearly indepdendent eigenvectors.
> In fact, $A = PDP^-1$, with $D$ a diagonal matrix, if and only if the columns
> of $P$ are $n$ linearly independent eigenvectors of $A$. In this case, the
> diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively,
> to the eigenvectors in $P$. (Lay 5.3)

<div style='background-color: #ebffd4; padding: 10px; border: 1px solid #ccc;'>
In other words...

-<b>$A$ is diagonalizable if and only if there are enough eigenvectors to form a
basis of $\mathbb{R}^n$</b>

We call such a basis an **eigenvector basis**.
</div>

**Theorem 6**

> An $n \times n$ matrix with $n$ distinct _eigenvalues_ is diagonalizable.

- However, it's  still possible for $A$ to be diagonalizable even where there
  are less than $n$ distince eigenvalues.

Tangent: spectral analysis of covariance matrices
=================================================

To get a better feel for what the eigenvalues and eigenvectors of covariance
matrices look like, let's look at a couple examples using fake data.

Matrices with correlated features
---------------------------------

### 3 variables (2 correlated)

First, we will create a vector with three columns, one of which is a multiple
of another.

```{r eigenvector_example}
set.seed(1)

dat <- matrix(rnorm(40), ncol=2)
dat <- cbind(1.0 * dat[,1], dat)

# visualization relationship between the columns
cor(dat)
pairs(dat, upper.panel=NULL)
```

Next, we will take compute the _square_ covariance matrix corresponding to our original 
_rectangular_ data matrix.

Because of the linear dependence in the columns of `dat`, there are only two
non-zero eigenvalues.

```{r}
# get the (square) covariance matrix for our data
covmat <- cov(dat)

# compute the eigenvectors / eigenvalues of the covariance matrix
eigen(covmat, symmetric=TRUE)
```

In the output, the `$vectors` components is a $p \times p$ matrix whose
_columns_ contain the eigen vectors of the input matrix, normalized to unit
length (see `?eigen`).

Notice that the first two _rows_ in the result matrix are the same, but
pointing in opposite directions. What do the rows mean in this context?..

### 4 variables (3 correlated)

What happens if we add another column that is a multiple of the first one?

```{r}
dat2 <- cbind(2 * dat[,1], dat)
cor(dat2)
eigen(cov(dat2), symmetric=TRUE)
```

Misc
====

- Nice quote from Lay in the opening of chapter 3:

> In Cauchy's day, when life was simple and matrices were small...

- While attempting to search for "eigendecomposition" and accidentally
searching for "eigendec" instead, this was the first hit that came up: 
[eigendec.m](https://github.com/CamDavidsonPilon/The-Golden-Retrieber/blob/master/eigendec.m)
from the project "The-Golden-Retrieber - A classification algorithm that classifies Justin
  Bieber in Twitter display pictures." ...

- Couple neat facts about eigenvalues and eigenvectors (via
  [Wikipedia](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)):
    1. The sum of the eigenvalues of a matrix equals the sum of its diagonal (its "trace")

```{r}
sum(diag(cor_mat1))
sum(eigen(cor_mat1)$values)
```
    2. The product of the eigenvalues of a matrix equals its determinant:

```{r}
prod(eigen(cor_mat1)$values)
det(cor_mat1)
```

    3. The eigenvectors of $A^-1$ are the same as the eigenvectors of $A$

- Finally, here is a nice animation taken from 
  [Wikipedia](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) which
  helps with some of the intuition between eigenvectors and eigenvalues:

![](img/Eigenvectors.gif)

System Info
===========

```{r}
sessionInfo()
```

References
==========

1. _Lay_ chapter 5.2 - 5.3
2. https://en.wikipedia.org/wiki/Determinant
3. http://math.stackexchange.com/questions/668/whats-an-intuitive-way-to-think-about-the-determinant
4. https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix
