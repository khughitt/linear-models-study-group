---
output:
  md_document:
    variant: markdown_github
    md_extensions: +tex_math_dollars
    pandoc_args: ['--webtex']
    toc: true
    toc_depth: 2
---

```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.path='img/',
               fig.width=1080/120,
               fig.height=1080/120,
               dpi=120, fig.retina=2)
```

<h1>Advanced Linear Models III</h1>

- Keith Hughitt
- April 18, 2017

Overview
========

Covering a couple of left-over sections from weeks 3-4:

- Week 3 Generalizations (textbook section 4.4: _Extension to other spaces_)
- Week 4 Projections (textbook section 5.4: _Projections_)

Week 3: Generalizations
=======================

So far, we have been considering linear regression in Euclidean space (ℝ^n). It
turns out, however, that many of the same approaches readily apply to other
more general vector spaces.

One such example is a [Hilbert Space](https://en.wikipedia.org/wiki/Hilbert_space).

- A Hilbert Space is a vector space generalization of the Euclidean space which
has an associted _inner product_ that allows lengths and angles to be measured.
- Used in functional analysis, quantum mechanics, Fourier analysis, etc.

## Linear regression for a function space

The course gives an example of extending linear regression to a space of square
integrable functions.

- Let _y_ be in the space of functions from $[0,1] \rightarrow ℝ$ with finite
  squared integral.
  - "In mathematics, a square-integrable function, also called a quadratically
    integrable function, is a real- or complex-valued measurable function for
    which the integral of the square of the absolute value is finite."
    (Wikipedia)
- Define the inner product as:

$$
\langle f,g \rangle = \int_{0}^{1} f(t)g(t) dt
$$

- In this case, we want to find the best approximation to y in x.
- The solution for $\hat{\beta}$ is the same as for linear regression with real
  numbers:

$$
\hat{\beta} = \over{\langle y,x \rangle}{\langle x,x \rangle}
$$

- Textbook/video demonstrate the above and also extend the example to include
  an intercept, and define functional variance and correlation.

- Functional Example:

Suppose you have functions a quadratic function $y(t)$ that you want to
approximate with a linear function:

$$
y(t) = t + 2t^2
x(t) = t
$$

Using the integral-based definition of the inner-product least-squares solution
above (integral of y(t) over the integral of x(t)), you arrive at a slope for
the best-fit line of 2.5.

```{r function_lm_example}
t <- seq(0, 1, by=1/1000)
y <- t + 2*t^2
x <- t

lm(y ~ x - 1)
```

*Note: what's the "-1" in the model equation above for?







References
==========

1. Advanced Linear Models by Brian Caffo (Coursera)
2. https://en.wikipedia.org/wiki/Hilbert_space
3. https://en.wikipedia.org/wiki/Square-integrable_function

