---
output:
  md_document:
    variant: markdown_github
    md_extensions: +tex_math_dollars
    pandoc_args: ['--webtex']
    toc: true
    toc_depth: 2
---

```{r include=FALSE}
library(knitr)
opts_chunk$set(fig.path='img/',
               fig.width=1080/120,
               fig.height=1080/120,
               dpi=120, fig.retina=2)
```

<h1>Advanced Linear Models III</h1>

- Keith Hughitt
- April 18, 2017

Overview
========

Covering a couple of left-over sections from weeks 3-4:

- Week 3 Generalizations (textbook section 4.4: _Extension to other spaces_)
- Week 4 Projections (textbook section 5.4: _Projections_)

Week 3: Generalizations
=======================

So far, we have been considering linear regression in Euclidean space ($ℝ^n$). It
turns out, however, that many of the same approaches readily apply to other
more general vector spaces.

One such example is a [Hilbert Space](https://en.wikipedia.org/wiki/Hilbert_space).

- A Hilbert Space is a vector space generalization of the Euclidean space which
has an associted _inner product_ that allows lengths and angles to be measured.
- Used in functional analysis, quantum mechanics, Fourier analysis, etc.

## Linear regression for a function space

The course gives an example of extending linear regression to a space of square
integrable functions.

- Let _y_ be in the space of functions from $[0,1] \rightarrow ℝ$ with finite
  squared integral.
  - "In mathematics, a square-integrable function, also called a quadratically
    integrable function, is a real- or complex-valued measurable function for
    which the integral of the square of the absolute value is finite."
    (Wikipedia)
- Define the inner product as:

$$
\langle f,g \rangle = \int_0^1 f(t)g(t) \mathrm{d}t
$$

- In this case, we want to find the best approximation to y in x.
- The solution for $\hat{\beta}$ is the same as for linear regression with real
  numbers:

$$
\hat{\beta} = \frac{\langle y,x \rangle}{\langle x,x \rangle}
$$

- Textbook/video demonstrate the above and also extend the example to include
  an intercept, and define functional variance and correlation.

- Functional Example:

Suppose you have functions a quadratic function $y(t)$ that you want to
approximate with a linear function:

$$
y(t) = t + 2t^2
\newline
x(t) = t
$$

Using the integral-based definition of the inner-product least-squares solution
above (integral of y(t) over the integral of x(t)), you arrive at a slope for
the best-fit line of 2.5.

```{r function_lm_example}
t <- seq(0, 1, by=1/1000)
y <- t + 2*t^2
x <- t

lm(y ~ x - 1)
```

*Note: what's the "-1" in the model equation above for?

Week 4: Projections
===================

## Projection matrix

Thinking of least squares in terms of projections.

Suppose you have:

$$
y in ℝ^3
\newline
x = 3x2 matrix
$$

Want to minimize:

$$
|| y - x\beta ||^2
$$

- y is a vector in $ℝ^3$
- the solution space for $\beta$ ($\gamma$) in the notes is a 2-dimensional
  subspace in $ℝ^3$
- The solution to the above ($\hat{y}$) is the point in $\gamma$ that is the
  minimum distance from $y$

An interesting result of this:

- Adding a redundant column to $x$ (example used: $x_1 + x_2$), doesn't change
  the solution space: 
    - Even with this new column, the rank of the matrix is the same, and, 
    - the space spanned (all possible linear columns of $x_1 \text{and} x_2$) 
      and the solution $\hat{y}$ are the same.
- This also means that we don't need an invertible matrix to find a solution
- We just need the space defined by the linearly indepedent _subset_ of the
  columns of $X$.
- Also, different design matrices (video uses examples of matrices with all 1's
  and 0's organized in different ways, ~5:45) also span the same $\gamma$.

Next, recall:

$$
\hat{\beta} = (x^Tx)^{-1}x^Ty
$$

That is the projection of $y$ on to the space defined by the linear
combinations of the columns of $x$.

The solution, $\hat{\beta}$ is the particular vector in $ℝ^2$ that converts
projects $x$ onto $\gamma$.

So the _projection_ is:

$$
\hat{\beta} = H_xy = x(x^Tx)^{-1}x^Ty
$$

The matrix $H_x$ (the "projection matrix" or "hat matrix") is the linear
operator that projects a vector in $ℝ^n$ onto the 2-dimensional space spanned by the
columns of $X$.

## Residuals

Residuals:

$$
e = y - \hat{y} = y - x(x^Tx)^{-1}x^Ty
\newline
  = (I - H_x)y
$$

- $e$ is orthogonal to any point in $\gamma$
- $e$ is orthogonal to any column of $X$
- The sum of the residuals equals 0.


References
==========

1. Advanced Linear Models by Brian Caffo (Coursera)
2. https://en.wikipedia.org/wiki/Hilbert_space
3. https://en.wikipedia.org/wiki/Square-integrable_function

